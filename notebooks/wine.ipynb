{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a6100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for building and training the neural network\n",
    "import torch  # PyTorch for tensor operations and neural network computations\n",
    "import matplotlib.pyplot as plt  # For creating visualisations and plots\n",
    "from sklearn.datasets import load_wine  # Load the wine classification dataset\n",
    "from sklearn.model_selection import train_test_split  # Split data into train/test sets\n",
    "from sklearn.preprocessing import StandardScaler  # Normalise features to have mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1517285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset from scikit-learn\n",
    "# This dataset contains 13 chemical measurements from 178 wine samples\n",
    "# across 3 different wine cultivars (classes 0, 1, and 2)\n",
    "X, y = load_wine(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Split the dataset into 70% training and 30% testing sets\n",
    "# random_state ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "# Standardise the features to have mean=0 and standard deviation=1\n",
    "# This is crucial for neural networks as it:\n",
    "# 1. Helps gradient descent converge faster\n",
    "# 2. Prevents features with larger scales from dominating the learning\n",
    "# 3. Keeps activations in a reasonable range\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "scaled_X_train = scaler.fit_transform(X_train)  # Fit scaler on training data and transform\n",
    "scaled_X_test = scaler.transform(X_test)  # Transform test data using training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64623602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Visualise feature correlations to understand relationships between chemical measurements\n",
    "# Strong correlations may indicate redundant features or multicollinearity\n",
    "\n",
    "# Combine scaled features with target labels for analysis\n",
    "train_data = scaled_X_train.copy()\n",
    "train_data['target'] = y_train.values\n",
    "\n",
    "# Compute the correlation matrix for all features\n",
    "# Values range from -1 (perfect negative correlation) to +1 (perfect positive correlation)\n",
    "corr_matrix = train_data.drop('target', axis=1).corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a mask to hide the upper triangle (matrix is symmetric)\n",
    "# This makes the heatmap cleaner and easier to read\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Plot the correlation heatmap\n",
    "# - annot=True displays correlation values in each cell\n",
    "# - fmt=\".2f\" formats values to 2 decimal places\n",
    "# - cmap='viridis' uses consistent colourmap with other plots\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    mask=mask, \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    cmap='viridis', \n",
    "    vmin=-1, \n",
    "    vmax=1\n",
    ")\n",
    "\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()  # Adjust spacing for better appearance\n",
    "plt.savefig('../figures/wine_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd91936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform Principal Component Analysis (PCA) to visualise high-dimensional data in 2D\n",
    "# PCA finds the directions of maximum variance in the data\n",
    "# This helps us understand if the 3 wine classes are linearly separable\n",
    "\n",
    "# Reduce the 13-dimensional feature space to 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(scaled_X_train)\n",
    "\n",
    "# Create the PCA visualisation\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot with wine classes coloured differently\n",
    "# - c=y_train colours points by their class label (0, 1, or 2)\n",
    "# - cmap='viridis' uses consistent colourmap\n",
    "# - edgecolor='k' adds black borders to make points more visible\n",
    "# - alpha=0.7 adds slight transparency for overlapping points\n",
    "scatter = plt.scatter(\n",
    "    X_pca[:, 0], \n",
    "    X_pca[:, 1], \n",
    "    c=y_train, \n",
    "    cmap='viridis', \n",
    "    edgecolors='k', \n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Label axes with variance explained by each principal component\n",
    "# This tells us how much information each axis captures\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} Variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} Variance)')\n",
    "plt.title('PCA Projection of Wine Dataset')\n",
    "\n",
    "# Add a legend showing which colour corresponds to which wine class\n",
    "plt.legend(*scatter.legend_elements(), title=\"Wine Class\")\n",
    "\n",
    "# Add a subtle grid for easier reading\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing for better appearance\n",
    "plt.savefig('../figures/wine_pca.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b4fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise pairwise relationships between selected features\n",
    "# A pairplot shows scatter plots for each feature pair and distributions on the diagonal\n",
    "# This helps identify which feature combinations best separate the wine classes\n",
    "\n",
    "# Select a subset of features that show strong class separation\n",
    "# (Chosen based on the correlation matrix and domain knowledge)\n",
    "features_of_interest = ['alcohol', 'flavanoids', 'color_intensity', 'proline']\n",
    "subset_data = train_data[features_of_interest + ['target']]\n",
    "\n",
    "# Create pairplot with:\n",
    "# - hue='target' colours points by wine class\n",
    "# - palette='viridis' for consistent colour scheme\n",
    "# - corner=True only shows lower triangle (reduces redundancy)\n",
    "# - diag_kind='kde' shows probability density on diagonal\n",
    "sns.pairplot(\n",
    "    subset_data, \n",
    "    hue='target', \n",
    "    palette='viridis', \n",
    "    corner=True, \n",
    "    diag_kind='kde'\n",
    ")\n",
    "\n",
    "plt.suptitle(\"Pairwise Feature Relationships\", y=1.02)\n",
    "plt.savefig('../figures/wine_pairplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Visualise all features simultaneously using parallel coordinates\n",
    "# Each vertical line represents one feature, and each coloured line represents one wine sample\n",
    "# This plot helps identify which features contribute most to class separation\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create parallel coordinates plot\n",
    "# - Each wine sample is a line connecting its feature values\n",
    "# - Lines are coloured by wine class (target)\n",
    "# - colormap='viridis' for consistent styling\n",
    "# - alpha=0.5 adds transparency to see overlapping patterns\n",
    "parallel_coordinates(\n",
    "    train_data, \n",
    "    'target', \n",
    "    colormap='viridis', \n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.title(\"Multivariate Feature Profiles\")\n",
    "plt.xticks(rotation=45)  # Rotate feature names for readability\n",
    "plt.grid(True, alpha=0.3)  # Add subtle grid\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing for better appearance\n",
    "plt.savefig('../figures/wine_parallel_coordinates.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Select the best available computing device (GPU if available, otherwise CPU)\n",
    "# GPUs can significantly speed up neural network training through parallel processing\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors and move to device\n",
    "# float32 is used for efficiency (GPUs are optimised for 32-bit floats)\n",
    "# long is used for class labels in multi-class classification\n",
    "X_train_t = torch.tensor(scaled_X_train.values, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train.values, dtype=torch.long).to(device)\n",
    "X_test_t = torch.tensor(scaled_X_test.values, dtype=torch.float32).to(device)\n",
    "y_test_t = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "\n",
    "# Define a simple Multi-Layer Perceptron (MLP) architecture\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward neural network with one hidden layer.\n",
    "    \n",
    "    Architecture:\n",
    "        Input Layer (13 features) -> Hidden Layer (64 neurones, ReLU) -> Output Layer (3 classes)\n",
    "    \n",
    "    Args:\n",
    "        n_input (int): Number of input features\n",
    "        n_hidden (int): Number of neurones in the hidden layer\n",
    "        n_output (int): Number of output classes\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # First layer: transforms input features to hidden layer\n",
    "        self.layer1 = nn.Linear(n_input, n_hidden)\n",
    "        # ReLU activation introduces non-linearity\n",
    "        # ReLU(x) = max(0, x) - simple but effective\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second layer: transforms hidden layer to output logits\n",
    "        self.layer2 = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input data\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Raw output scores (logits) for each class\n",
    "        \"\"\"\n",
    "        # Pass through first layer, apply ReLU, then pass through second layer\n",
    "        return self.layer2(self.relu(self.layer1(x)))\n",
    "\n",
    "# Create model with:\n",
    "# - 13 input features (chemical measurements)\n",
    "# - 64 hidden neurones (enough capacity to learn wine patterns)\n",
    "# - 3 output classes (three wine cultivars)\n",
    "model = SimpleMLP(13, 64, 3).to(device)\n",
    "\n",
    "# Define loss function and optimiser\n",
    "# CrossEntropyLoss combines softmax and negative log-likelihood\n",
    "# It's the standard loss for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimiser adapts learning rate for each parameter\n",
    "# lr=0.01 is a good starting learning rate for most problems\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(20000):\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    # Forward pass: compute predictions\n",
    "    logits = model(X_train_t)\n",
    "    \n",
    "    # Compute loss: how far are predictions from true labels?\n",
    "    loss = criterion(logits, y_train_t)\n",
    "    \n",
    "    # Backward pass: compute gradients\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()  # Compute new gradients via backpropagation\n",
    "    optimizer.step()  # Update weights using gradients\n",
    "    \n",
    "    # Store loss for plotting\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    # Print progress every 5000 epochs\n",
    "    if epoch % 5000 == 0:\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            test_logits = model(X_test_t)\n",
    "            # Get predictions by selecting class with highest score\n",
    "            predictions = torch.argmax(test_logits, dim=1)\n",
    "            # Calculate accuracy\n",
    "            acc = (predictions == y_test_t).float().mean()\n",
    "            print(f\"Epoch {epoch:5d} | Loss: {loss.item():.8f} | Test Acc: {acc:.1%}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_preds = torch.argmax(model(X_test_t), dim=1)\n",
    "    final_acc = (final_preds == y_test_t).float().mean()\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {final_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the training loss over time to assess learning progress\n",
    "# A decreasing loss indicates the model is successfully learning the patterns\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label='Training Loss')  # Plot loss values for each epoch\n",
    "plt.xlabel('Epoch')  # X-axis: training iteration number\n",
    "plt.ylabel('Cross-Entropy Loss')  # Y-axis: loss value\n",
    "\n",
    "# Use logarithmic scale on y-axis to better visualise exponential decay\n",
    "# This makes it easier to see improvements even when loss becomes very small\n",
    "plt.yscale('log')\n",
    "\n",
    "# Add a subtle grid for easier reading of values\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing for better appearance\n",
    "plt.savefig('../figures/wine_training_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8905dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Prepare predictions and probabilities for evaluation\n",
    "# We need both hard predictions (class labels) and soft predictions (probabilities)\n",
    "# for different types of analysis\n",
    "\n",
    "# Ensure the model is in evaluation mode (disables dropout, batch norm, etc.)\n",
    "model.eval()\n",
    "\n",
    "# Compute predictions without tracking gradients (saves memory and computation)\n",
    "with torch.no_grad():\n",
    "    # Get raw output scores (logits) from the model\n",
    "    test_logits = model(X_test_t)\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    # Softmax ensures probabilities sum to 1: p_i = exp(logit_i) / sum(exp(logit_j))\n",
    "    test_probs = torch.softmax(test_logits, dim=1)\n",
    "    \n",
    "    # Get hard predictions by selecting the class with highest probability\n",
    "    test_preds = torch.argmax(test_logits, dim=1)\n",
    "\n",
    "# Move predictions from GPU/device memory to CPU for use with scikit-learn\n",
    "# scikit-learn expects NumPy arrays, not PyTorch tensors\n",
    "y_true = y_test_t.cpu().numpy()  # True labels\n",
    "y_pred = test_preds.cpu().numpy()  # Predicted labels\n",
    "y_prob = test_probs.cpu().numpy()  # Class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d84355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix to visualise classification performance\n",
    "# The confusion matrix shows how often each class is correctly or incorrectly classified\n",
    "# Diagonal elements = correct predictions, off-diagonal = mistakes\n",
    "\n",
    "# Compute the confusion matrix\n",
    "# cm[i,j] = number of samples with true class i predicted as class j\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get class names from the dataset for better labelling\n",
    "class_labels = load_wine().target_names\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "\n",
    "# Plot confusion matrix as a heatmap\n",
    "# - annot=True displays count values in each cell\n",
    "# - fmt='d' formats values as integers\n",
    "# - cmap='viridis' uses consistent colour scheme\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='viridis',\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels\n",
    ")\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing for better appearance\n",
    "plt.savefig('../figures/wine_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for multi-class classification\n",
    "# ROC (Receiver Operating Characteristic) curves show the trade-off between\n",
    "# true positive rate and false positive rate at different classification thresholds\n",
    "# AUC (Area Under Curve) summarises performance: 1.0 = perfect, 0.5 = random\n",
    "\n",
    "# Convert true labels to binary format for one-vs-rest ROC analysis\n",
    "# For class 0: [1, 0, 0], for class 1: [0, 1, 0], etc.\n",
    "y_true_bin = label_binarize(y_true, classes=[0, 1, 2])\n",
    "n_classes = y_true_bin.shape[1]\n",
    "\n",
    "# Storage for ROC curve data for each class\n",
    "fpr = dict()  # False positive rate\n",
    "tpr = dict()  # True positive rate\n",
    "roc_auc = dict()  # Area under the ROC curve\n",
    "\n",
    "# Compute ROC curve for each wine class\n",
    "for i in range(n_classes):\n",
    "    # Compare binary true labels vs. predicted probabilities for this class\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "    # Calculate area under the ROC curve (higher is better)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Create the ROC curve plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "colours = ['#440154', '#21918c', '#fde724']  # Viridis-inspired colours\n",
    "for i in range(n_classes):\n",
    "    plt.plot(\n",
    "        fpr[i],\n",
    "        tpr[i],\n",
    "        color=colours[i],\n",
    "        lw=2,\n",
    "        label=f'{class_labels[i]} (AUC = {roc_auc[i]:.2f})',\n",
    "    )\n",
    "\n",
    "# Add diagonal reference line (random classifier baseline)\n",
    "# A random classifier has AUC = 0.5\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves (One-vs-Rest)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)  # Add subtle grid\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing for better appearance\n",
    "plt.savefig('../figures/wine_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u73xjau7c2p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise hidden layer activations to understand what the network learnt\n",
    "# Each of the 64 hidden neurones learns to detect different chemical patterns\n",
    "# By examining activations, we can see how the network internally represents different wines\n",
    "\n",
    "# Extract hidden layer activations by using a forward hook\n",
    "# A hook allows us to capture intermediate layer outputs during forward pass\n",
    "hidden_activations = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"Hook function to capture ReLU output (hidden layer activations)\"\"\"\n",
    "    hidden_activations.append(output.detach().cpu())\n",
    "\n",
    "# Register the hook on the ReLU layer (which outputs the hidden layer activations)\n",
    "hook = model.relu.register_forward_hook(hook_fn)\n",
    "\n",
    "# Perform forward pass to trigger the hook and collect activations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _ = model(X_test_t)\n",
    "\n",
    "# Remove the hook (good practice to clean up)\n",
    "hook.remove()\n",
    "\n",
    "# Extract activations as a numpy array\n",
    "# Shape: (n_test_samples, n_hidden_neurones)\n",
    "activations = hidden_activations[0].numpy()\n",
    "\n",
    "# Convert labels to numpy for easier indexing\n",
    "y_test_np = y_test_t.cpu().numpy()\n",
    "\n",
    "print(f\"Activation matrix shape: {activations.shape}\")\n",
    "print(f\"Hidden layer has {activations.shape[1]} neurones\")\n",
    "print(f\"Test set has {activations.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k0nioz7lwi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive heatmap showing activation patterns for all wines\n",
    "# This reveals which neurones are most active for each wine class\n",
    "\n",
    "# Sort samples by class for clearer visualisation\n",
    "sort_indices = np.argsort(y_test_np)\n",
    "sorted_activations = activations[sort_indices]\n",
    "sorted_labels = y_test_np[sort_indices]\n",
    "\n",
    "# Find boundaries between classes\n",
    "class_0_end = np.sum(sorted_labels == 0)\n",
    "class_1_end = class_0_end + np.sum(sorted_labels == 1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create heatmap where:\n",
    "# - Each column is a test sample (wine)\n",
    "# - Each row is a hidden neurone\n",
    "# - Colour intensity shows activation strength (ReLU output, so 0 or positive)\n",
    "plt.imshow(sorted_activations.T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "cbar = plt.colorbar(label='Activation Level')\n",
    "\n",
    "# Add vertical lines to separate the three wine classes\n",
    "plt.axvline(x=class_0_end - 0.5, color='red', linestyle='--', linewidth=2)\n",
    "plt.axvline(x=class_1_end - 0.5, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Add class labels at the top\n",
    "class_0_centre = class_0_end / 2\n",
    "class_1_centre = class_0_end + (class_1_end - class_0_end) / 2\n",
    "class_2_centre = class_1_end + (len(sorted_labels) - class_1_end) / 2\n",
    "\n",
    "plt.text(class_0_centre, -3, class_labels[0], ha='center', fontsize=12, fontweight='bold')\n",
    "plt.text(class_1_centre, -3, class_labels[1], ha='center', fontsize=12, fontweight='bold')\n",
    "plt.text(class_2_centre, -3, class_labels[2], ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Test Samples (sorted by wine class)')\n",
    "plt.ylabel('Hidden Neurone')\n",
    "plt.title('Hidden Layer Activation Heatmap\\nEach wine class activates different neurone patterns')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/wine_activation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lqfwmt8zqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse which neurones are most important for each wine class\n",
    "# By averaging activations per class, we can identify class-specific neurones\n",
    "\n",
    "# Compute average activation for each class\n",
    "avg_activations = np.zeros((3, activations.shape[1]))\n",
    "for class_idx in range(3):\n",
    "    class_mask = y_test_np == class_idx\n",
    "    avg_activations[class_idx] = activations[class_mask].mean(axis=0)\n",
    "\n",
    "# Create bar plot showing average activation per class\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for class_idx in range(3):\n",
    "    ax = axes[class_idx]\n",
    "    \n",
    "    # Create bar plot of average neurone activations for this wine class\n",
    "    bars = ax.bar(\n",
    "        range(64), \n",
    "        avg_activations[class_idx],\n",
    "        color='#440154',  # Viridis dark colour\n",
    "        alpha=0.7,\n",
    "        edgecolor='k'\n",
    "    )\n",
    "    \n",
    "    # Highlight the top 5 most active neurones for this class\n",
    "    top_5_neurones = np.argsort(avg_activations[class_idx])[-5:]\n",
    "    for neurone_idx in top_5_neurones:\n",
    "        bars[neurone_idx].set_color('#fde724')  # Viridis bright colour\n",
    "    \n",
    "    ax.set_xlabel('Hidden Neurone')\n",
    "    ax.set_ylabel('Average Activation')\n",
    "    ax.set_title(f'{class_labels[class_idx]}\\n(Top 5 neurones highlighted)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_xlim(-1, 64)\n",
    "\n",
    "plt.suptitle('Average Hidden Layer Activation by Wine Class', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/wine_avg_activation_by_class.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print the top 5 most discriminative neurones for each class\n",
    "print(\"Top 5 most active neurones for each wine class:\\n\")\n",
    "for class_idx in range(3):\n",
    "    top_neurones = np.argsort(avg_activations[class_idx])[-5:][::-1]\n",
    "    print(f\"{class_labels[class_idx]}:\")\n",
    "    for rank, neurone_idx in enumerate(top_neurones, 1):\n",
    "        activation_val = avg_activations[class_idx, neurone_idx]\n",
    "        print(f\"  {rank}. Neurone {neurone_idx:2d}: {activation_val:.3f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
