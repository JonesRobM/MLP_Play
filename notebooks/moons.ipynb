{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36887eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for building and training the neural network\n",
    "import torch  # PyTorch for tensor operations and neural network computations\n",
    "import matplotlib.pyplot as plt  # For creating visualisations and plots\n",
    "from sklearn.datasets import make_moons  # Generate non-linearly separable dataset\n",
    "from sklearn.model_selection import train_test_split  # Split data into train/test sets\n",
    "from sklearn.preprocessing import StandardScaler  # Normalise features to have mean=0, std=1\n",
    "import numpy as np  # Numerical operations on arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a non-linearly separable \"moons\" dataset for binary classification\n",
    "# This dataset consists of two interleaving half circles, which requires a \n",
    "# non-linear decision boundary that a simple linear classifier cannot learn\n",
    "X, y = make_moons(n_samples=10000, noise=0.2, random_state=42)\n",
    "\n",
    "# Split the dataset into 80% training and 20% testing sets\n",
    "# random_state ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=420)\n",
    "\n",
    "# Standardise the features to have mean=0 and standard deviation=1\n",
    "# This is crucial for neural networks as it:\n",
    "# 1. Helps gradient descent converge faster\n",
    "# 2. Prevents features with larger scales from dominating the learning\n",
    "# 3. Keeps activations in a reasonable range\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Fit scaler on training data and transform\n",
    "X_test = scaler.transform(X_test)  # Transform test data using training statistics\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors for neural network operations\n",
    "# float32 is used for efficiency (GPUs are optimised for 32-bit floats)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)  # Reshape to column vector\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)  # Reshape to column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the training and testing datasets side-by-side\n",
    "# This helps us understand the data distribution and verify the split\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot training data\n",
    "# - c=y_train.squeeze() colours points by their class label\n",
    "# - cmap='viridis' uses a perceptually uniform colourmap\n",
    "# - edgecolors='k' adds black borders to make points more visible\n",
    "# - alpha=0.7 adds slight transparency\n",
    "ax1.scatter(X_train[:, 0], X_train[:, 1], c=y_train.squeeze(), cmap='viridis', edgecolors='k', alpha=0.7)\n",
    "ax1.set_title('Training Dataset')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "\n",
    "# Plot testing data with the same styling\n",
    "# This allows us to visually compare if the test set has a similar distribution\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=y_test.squeeze(), cmap='viridis', edgecolors='k', alpha=0.7)\n",
    "ax2.set_title('Testing Dataset')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing between subplots for better appearance\n",
    "plt.savefig('../figures/moons_dataset.png', dpi=300, bbox_inches='tight')  # Save the figure to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567431f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    \"\"\"\n",
    "    A simple Multi-Layer Perceptron (MLP) neural network with one hidden layer.\n",
    "    \n",
    "    This implementation uses manual backpropagation to update weights and biases,\n",
    "    demonstrating the fundamental mechanics of neural network training without\n",
    "    relying on automatic differentiation frameworks.\n",
    "    \n",
    "    Architecture:\n",
    "        Input Layer -> Hidden Layer (sigmoid) -> Output Layer (sigmoid)\n",
    "    \n",
    "    Attributes:\n",
    "        W1 (torch.Tensor): Weight matrix for input to hidden layer (input_size x hidden_size)\n",
    "        b1 (torch.Tensor): Bias vector for hidden layer (1 x hidden_size)\n",
    "        W2 (torch.Tensor): Weight matrix for hidden to output layer (hidden_size x output_size)\n",
    "        b2 (torch.Tensor): Bias vector for output layer (1 x output_size)\n",
    "        z1 (torch.Tensor): Pre-activation values for hidden layer\n",
    "        a1 (torch.Tensor): Activated values for hidden layer\n",
    "        z2 (torch.Tensor): Pre-activation values for output layer\n",
    "        a2 (torch.Tensor): Activated values for output layer (final predictions)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialise the MLP with random weights and biases.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Number of input features\n",
    "            hidden_size (int): Number of neurones in the hidden layer\n",
    "            output_size (int): Number of output neurones (typically 1 for binary classification)\n",
    "        \"\"\"\n",
    "        # Initialise weights randomly from a standard normal distribution\n",
    "        # requires_grad=True enables automatic gradient computation (though we compute manually)\n",
    "        self.W1 = torch.randn(input_size, hidden_size, requires_grad=True)\n",
    "        self.b1 = torch.randn(1, hidden_size, requires_grad=True)\n",
    "        self.W2 = torch.randn(hidden_size, output_size, requires_grad=True)\n",
    "        self.b2 = torch.randn(1, output_size, requires_grad=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "        \n",
    "        This method computes the network's predictions by:\n",
    "        1. Computing hidden layer pre-activations: z1 = X @ W1 + b1\n",
    "        2. Applying sigmoid activation: a1 = sigmoid(z1)\n",
    "        3. Computing output layer pre-activations: z2 = a1 @ W2 + b2\n",
    "        4. Applying sigmoid activation: a2 = sigmoid(z2)\n",
    "        \n",
    "        Args:\n",
    "            X (torch.Tensor): Input data of shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Network predictions of shape (batch_size, output_size)\n",
    "                         Values are in range [0, 1] due to sigmoid activation\n",
    "        \"\"\"\n",
    "        # Hidden layer: Linear transformation followed by sigmoid activation\n",
    "        # z1 = X * W1 + b1 (matrix multiplication)\n",
    "        self.z1 = torch.matmul(X, self.W1) + self.b1\n",
    "        \n",
    "        # Apply sigmoid activation: σ(z) = 1 / (1 + e^(-z))\n",
    "        # This introduces non-linearity, allowing the network to learn complex patterns\n",
    "        self.a1 = torch.sigmoid(self.z1)\n",
    "        \n",
    "        # Output layer: Linear transformation of hidden layer activations\n",
    "        # z2 = a1 * W2 + b2\n",
    "        self.z2 = torch.matmul(self.a1, self.W2) + self.b2\n",
    "        \n",
    "        # Apply sigmoid to get final predictions in range [0, 1]\n",
    "        # For binary classification, we can interpret this as P(class=1)\n",
    "        self.a2 = torch.sigmoid(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output, lr=0.01):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to compute gradients and update weights.\n",
    "        \n",
    "        This method implements the backpropagation algorithm manually:\n",
    "        1. Computes gradients for output layer (dW2, db2)\n",
    "        2. Propagates error back to hidden layer\n",
    "        3. Computes gradients for hidden layer (dW1, db1)\n",
    "        4. Updates all weights and biases using gradient descent\n",
    "        \n",
    "        The gradient calculations assume Mean Squared Error (MSE) loss and\n",
    "        sigmoid activations throughout the network.\n",
    "        \n",
    "        Args:\n",
    "            X (torch.Tensor): Input data of shape (batch_size, input_size)\n",
    "            y (torch.Tensor): True labels of shape (batch_size, output_size)\n",
    "            output (torch.Tensor): Network predictions from forward pass\n",
    "            lr (float): Learning rate for gradient descent (default: 0.01)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # Number of training examples in the batch\n",
    "        \n",
    "        # ===== BACKPROPAGATION THROUGH OUTPUT LAYER =====\n",
    "        # For MSE loss L = (1/2m) * Σ(output - y)^2\n",
    "        # dL/dz2 = output - y (derivative of MSE w.r.t. pre-activation)\n",
    "        dz2 = output - y\n",
    "        \n",
    "        # Gradient of loss w.r.t. W2: dL/dW2 = a1^T * dz2\n",
    "        # This tells us how much to adjust W2 to reduce the loss\n",
    "        dW2 = torch.matmul(self.a1.T, dz2)\n",
    "        \n",
    "        # Gradient of loss w.r.t. b2: dL/db2 = average of dz2 across batch\n",
    "        db2 = torch.sum(dz2, axis=0) / m\n",
    "\n",
    "        # ===== BACKPROPAGATION THROUGH HIDDEN LAYER =====\n",
    "        # Propagate error backward to the hidden layer\n",
    "        # dL/da1 = dz2 * W2^T (chain rule)\n",
    "        da1 = torch.matmul(dz2, self.W2.T)\n",
    "        \n",
    "        # Apply derivative of sigmoid activation: σ'(z) = σ(z) * (1 - σ(z))\n",
    "        # dL/dz1 = dL/da1 * da1/dz1 = da1 * σ'(z1)\n",
    "        dz1 = da1 * (self.a1 * (1 - self.a1))\n",
    "        \n",
    "        # Gradient of loss w.r.t. W1: dL/dW1 = X^T * dz1 / m\n",
    "        dw1 = torch.matmul(X.T, dz1) / m\n",
    "        \n",
    "        # Gradient of loss w.r.t. b1: dL/db1 = average of dz1 across batch\n",
    "        db1 = torch.sum(dz1, axis=0) / m\n",
    "        \n",
    "        # ===== GRADIENT DESCENT UPDATE =====\n",
    "        # Update weights and biases: θ_new = θ_old - learning_rate * gradient\n",
    "        # torch.no_grad() disables gradient tracking for efficiency\n",
    "        with torch.no_grad():\n",
    "            self.W1 -= lr * dw1  # Update input-to-hidden weights\n",
    "            self.b1 -= lr * db1  # Update hidden layer biases\n",
    "            self.W2 -= lr * dW2  # Update hidden-to-output weights\n",
    "            self.b2 -= lr * db2  # Update output layer biases\n",
    "            \n",
    "    def train(self, X, y, epochs=1000, lr=0.01):\n",
    "        \"\"\"\n",
    "        Train the neural network using gradient descent.\n",
    "        \n",
    "        This method performs the complete training loop:\n",
    "        1. Forward pass to compute predictions\n",
    "        2. Calculate Mean Squared Error (MSE) loss\n",
    "        3. Backward pass to update weights\n",
    "        4. Track and print loss at regular intervals\n",
    "        \n",
    "        Args:\n",
    "            X (torch.Tensor): Training input data of shape (batch_size, input_size)\n",
    "            y (torch.Tensor): Training labels of shape (batch_size, output_size)\n",
    "            epochs (int): Number of training iterations (default: 1000)\n",
    "            lr (float): Learning rate for gradient descent (default: 0.01)\n",
    "        \n",
    "        Returns:\n",
    "            list: Loss values for each epoch, useful for plotting training progress\n",
    "        \"\"\"\n",
    "        losses = []  # Track loss history for visualisation\n",
    "        \n",
    "        # Training loop: iterate over the entire dataset multiple times\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass: compute predictions\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Compute Mean Squared Error (MSE) loss\n",
    "            # MSE = (1/m) * Σ(prediction - actual)^2\n",
    "            # Lower loss indicates better model performance\n",
    "            loss = torch.mean((output - y) ** 2)\n",
    "            losses.append(loss.item())  # Store loss value (convert to Python float)\n",
    "            \n",
    "            # Backward pass: compute gradients and update weights\n",
    "            self.backward(X, y, output, lr)\n",
    "            \n",
    "            # Print progress every 1000 epochs to monitor training\n",
    "            if (epoch + 1) % 5000 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "                \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079101b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "input_size = 2  # Two features (x and y coordinates from the moons dataset)\n",
    "hidden_size = 4  # Four neurones in the hidden layer (enough to learn the moons pattern)\n",
    "output_size = 1  # Single output for binary classification (probability of class 1)\n",
    "\n",
    "# Create an instance of the SimpleMLP model\n",
    "model = SimpleMLP(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the model on the training data\n",
    "# epochs=50000: Run 50,000 iterations over the entire training set\n",
    "# lr=0.1: Use a relatively high learning rate for faster convergence\n",
    "# The model will learn to separate the two moon-shaped classes\n",
    "losses = model.train(X_train, y_train, epochs=20000, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677594f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the training loss over time to assess learning progress\n",
    "# A decreasing loss indicates the model is successfully learning the pattern\n",
    "\n",
    "plt.plot(losses)  # Plot loss values for each epoch\n",
    "plt.xlabel(\"Epoch\")  # X-axis: training iteration number\n",
    "plt.ylabel(\"Loss\")  # Y-axis: Mean Squared Error loss value\n",
    "\n",
    "# Use logarithmic scale on y-axis to better visualise exponential decay\n",
    "# This makes it easier to see improvements even when loss becomes very small\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "# Set specific tick marks for clearer reading\n",
    "plt.yticks([0.01, 0.1, 1], ['0.01', '0.1', '1'])\n",
    "\n",
    "# Add a subtle grid for easier reading of values\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../figures/moon_training_loss.png', dpi=300, bbox_inches='tight')  # Save the figure to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fbdd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the test set\n",
    "# This measures how well the model generalises to unseen data\n",
    "\n",
    "# Disable gradient computation for inference (saves memory and computation)\n",
    "with torch.no_grad():\n",
    "    # Get model predictions on test data\n",
    "    test_output = model.forward(X_test)\n",
    "    \n",
    "    # Convert probabilities to binary predictions using 0.5 threshold\n",
    "    # If output > 0.5, predict class 1; otherwise predict class 0\n",
    "    test_output = (test_output > 0.5).float()\n",
    "\n",
    "# Calculate accuracy: percentage of correct predictions\n",
    "# Compares predicted labels with true labels and computes the mean\n",
    "accuracy = torch.mean((test_output == y_test).float())\n",
    "\n",
    "# Display the test accuracy as a percentage\n",
    "print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a635c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the decision boundary learnt by the MLP\n",
    "# This shows how the model divides the feature space into two classes\n",
    "\n",
    "# Create a mesh grid covering the entire feature space\n",
    "# This grid will be used to visualise the decision boundary\n",
    "x_min, x_max = X_test[:, 0].min() - 0.5, X_test[:, 0].max() + 0.5  # Add padding for visualisation\n",
    "y_min, y_max = X_test[:, 1].min() - 0.5, X_test[:, 1].max() + 0.5\n",
    "h = 0.02  # Step size in the mesh (smaller = higher resolution, but slower)\n",
    "\n",
    "# Create a 2D grid of points covering the feature space\n",
    "# Each point will be classified to show the decision regions\n",
    "xx, yy = torch.meshgrid(torch.arange(x_min, x_max, h), torch.arange(y_min, y_max, h), indexing='ij')\n",
    "\n",
    "# Flatten the grid into a list of (x, y) coordinates\n",
    "# This converts the 2D grid into a format the model can process\n",
    "grid_points = torch.stack([xx.ravel(), yy.ravel()], dim=1)\n",
    "\n",
    "# Predict class probabilities for every point in the grid\n",
    "with torch.no_grad():\n",
    "    Z = model.forward(grid_points)  # Get predictions for all grid points\n",
    "    Z = Z.reshape(xx.shape)  # Reshape back to 2D grid for visualisation\n",
    "\n",
    "# Create side-by-side comparison of model predictions vs ground truth\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ===== LEFT SUBPLOT: MLP PREDICTIONS =====\n",
    "# Show decision regions as coloured background\n",
    "# Darker regions = model predicts class 0, lighter = class 1\n",
    "ax1.contourf(xx, yy, Z, levels=20, cmap='viridis', alpha=0.6)\n",
    "\n",
    "# Draw the decision boundary (where probability = 0.5) in red\n",
    "# This line separates the two predicted classes\n",
    "ax1.contour(xx, yy, Z, levels=[0.5], colors='red', linewidths=2)\n",
    "\n",
    "# Overlay the actual test points with their PREDICTED labels\n",
    "# This shows which points were classified correctly (match ground truth)\n",
    "ax1.scatter(X_test[:, 0], X_test[:, 1], c=test_output.squeeze(), cmap='viridis', edgecolors='k', alpha=0.8)\n",
    "ax1.set_title('MLP Labels')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "\n",
    "# ===== RIGHT SUBPLOT: GROUND TRUTH =====\n",
    "# Use the same decision boundary for comparison\n",
    "ax2.contourf(xx, yy, Z, levels=20, cmap='viridis', alpha=0.6)\n",
    "ax2.contour(xx, yy, Z, levels=[0.5], colors='red', linewidths=2)\n",
    "\n",
    "# Overlay the actual test points with their TRUE labels\n",
    "# Comparing left and right plots shows where the model makes mistakes\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=y_test.squeeze(), cmap='viridis', edgecolors='k', alpha=0.8)\n",
    "ax2.set_title('Ground Truth')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing for better appearance\n",
    "plt.savefig('../figures/moon_decision_boundary.png', dpi=300, bbox_inches='tight')  # Save the figure to a file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jm6wk3udki",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise hidden layer activations to understand what the network learnt\n",
    "# Each hidden neuron learns to detect different patterns in the input data\n",
    "# By examining activations, we can see how the network internally represents the data\n",
    "\n",
    "# Select a subset of test samples for visualisation (balanced across classes)\n",
    "n_samples_per_class = 50\n",
    "class_0_indices = (y_test.squeeze() == 0).nonzero(as_tuple=True)[0][:n_samples_per_class]\n",
    "class_1_indices = (y_test.squeeze() == 1).nonzero(as_tuple=True)[0][:n_samples_per_class]\n",
    "sample_indices = torch.cat([class_0_indices, class_1_indices])\n",
    "\n",
    "# Get the samples and their labels\n",
    "X_sample = X_test[sample_indices]\n",
    "y_sample = y_test[sample_indices]\n",
    "\n",
    "# Perform forward pass and extract hidden layer activations\n",
    "# model.a1 contains the activated values from the hidden layer (after sigmoid)\n",
    "with torch.no_grad():\n",
    "    _ = model.forward(X_sample)\n",
    "    hidden_activations = model.a1.numpy()  # Shape: (n_samples, hidden_size)\n",
    "\n",
    "# Create visualisation showing how each hidden neuron responds to different inputs\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Plot each of the 4 hidden neurons separately\n",
    "for neuron_idx in range(4):\n",
    "    ax = axes[neuron_idx]\n",
    "    \n",
    "    # Create a scatter plot coloured by the activation level of this neuron\n",
    "    # Higher activation = neuron is \"interested\" in this input\n",
    "    scatter = ax.scatter(\n",
    "        X_sample[:, 0], \n",
    "        X_sample[:, 1], \n",
    "        c=hidden_activations[:, neuron_idx],\n",
    "        cmap='viridis',\n",
    "        edgecolors='k',\n",
    "        alpha=0.7,\n",
    "        s=50\n",
    "    )\n",
    "    \n",
    "    # Add a colourbar to show activation scale (0 to 1 for sigmoid)\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Activation Level')\n",
    "    \n",
    "    ax.set_title(f'Hidden Neuron {neuron_idx + 1} Activation')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Hidden Layer Activation Patterns', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/moon_hidden_activations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create a heatmap showing activation patterns across all samples and neurons\n",
    "# This gives a comprehensive view of how the hidden layer represents the data\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Sort samples by class for clearer visualisation\n",
    "sort_indices = torch.argsort(y_sample.squeeze())\n",
    "sorted_activations = hidden_activations[sort_indices]\n",
    "sorted_labels = y_sample[sort_indices].squeeze().numpy()\n",
    "\n",
    "# Create heatmap where:\n",
    "# - Each row is a test sample\n",
    "# - Each column is a hidden neuron\n",
    "# - Colour intensity shows activation strength\n",
    "plt.imshow(sorted_activations.T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Activation Level')\n",
    "\n",
    "# Add a vertical line to separate the two classes\n",
    "class_boundary = np.sum(sorted_labels == 0)\n",
    "plt.axvline(x=class_boundary - 0.5, color='red', linestyle='--', linewidth=2, label='Class Boundary')\n",
    "\n",
    "plt.xlabel('Test Samples (sorted by class)')\n",
    "plt.ylabel('Hidden Neuron')\n",
    "plt.title('Hidden Layer Activation Heatmap\\n(Class 0 | Class 1)')\n",
    "plt.yticks(range(4), [f'Neuron {i+1}' for i in range(4)])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/moon_activation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
